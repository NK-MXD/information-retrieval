{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对dataset当中的文本进行处理, 基本的处理方式:\n",
    "# 1. 将字母转化为小写(注: 作者名不进行转化);\n",
    "# 2. 将标点符号去掉;\n",
    "# 3. 将单词还原为原型;\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import string\n",
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 获取单词的词性\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"..//dataset\"\n",
    "output_path = \".//newdata//\"\n",
    "for doc in os.listdir(input_path):\n",
    "    # 大小写处理\n",
    "    name = doc[:-4].lower() \n",
    "    # 存放位置处理\n",
    "    new_doc_path = os.path.join(output_path, name) \n",
    "    # 存储文本\n",
    "    if os.path.exists(output_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(output_path)\n",
    "    shutil.copyfile(os.path.join(input_path,doc), new_doc_path)\n",
    "    author_flag = True\n",
    "    lines = []\n",
    "    for line in open(new_doc_path).readlines():\n",
    "        # 删除文本中的标点\n",
    "        line = line.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "        # 删除文本前后空格\n",
    "        line = line.strip()\n",
    "        # 对作者域进行处理\n",
    "        if author_flag:\n",
    "            line = line.lower()[7:]\n",
    "            author_flag = False\n",
    "        else:\n",
    "            # 大小写处理\n",
    "            line = line.lower()\n",
    "            # 对词形进行还原\n",
    "            words = line.split(' ')\n",
    "            # print(words)\n",
    "            word_list = []\n",
    "            tagged_sent = pos_tag(words)     # 获取单词词性\n",
    "            wnl = WordNetLemmatizer()\n",
    "            for tag in tagged_sent:\n",
    "                wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "                word_list.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) # 词形还原\n",
    "            line = ' '.join(word_list)\n",
    "            # print(line)\n",
    "        lines.append(line)\n",
    "    # 写入文件 其中Author在第一行, 第二行是对应的文本\n",
    "    with open(new_doc_path,\"w\") as f:\n",
    "        for i in range(len(lines)):\n",
    "            f.writelines(lines[i])\n",
    "            if i == 0:\n",
    "                f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面进行基于域的倒排索引记录表的建立\n",
    "# 先构建对应的词典类\n",
    "class IdMap:\n",
    "    def __init__(self):\n",
    "        self.str_to_id = {}\n",
    "        self.id_to_str = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of terms stored in the IdMap\"\"\"\n",
    "        return len(self.id_to_str)\n",
    "        \n",
    "    def _get_str(self, i):\n",
    "        \"\"\"Returns the string corresponding to a given id (`i`).\"\"\"\n",
    "        if i >= self.__len__():\n",
    "            raise IndexError\n",
    "        else:\n",
    "            return self.id_to_str[i]\n",
    "        \n",
    "    def _get_id(self, s):\n",
    "        \"\"\"Returns the id corresponding to a string (`s`). \n",
    "        If `s` is not in the IdMap yet, then assigns a new id and returns the new id.\n",
    "        \"\"\"\n",
    "        if s in self.str_to_id:\n",
    "            return self.str_to_id[s]\n",
    "        else:\n",
    "            self.id_to_str.append(s)\n",
    "            self.str_to_id[s]=len(self.id_to_str)-1\n",
    "            return self.str_to_id[s]\n",
    "            \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"If `key` is a integer, use _get_str; \n",
    "           If `key` is a string, use _get_id;\"\"\"\n",
    "        if type(key) is int:\n",
    "            return self._get_str(key)\n",
    "        elif type(key) is str:\n",
    "            return self._get_id(key)\n",
    "        else:\n",
    "            raise TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回忆倒排索引的建立过程: \n",
    "# 1.首先找到单个文本中的单词文档对 单个文本中文档有域属性\n",
    "# 2.需要统计每个单词在每个域中出现的次数\n",
    "class RegionIndex:\n",
    "    head = 1\n",
    "    author = 2\n",
    "    body = 4\n",
    "    def __init__(self, data_dir):\n",
    "        self.term_id_map = IdMap()\n",
    "        self.doc_id_map = IdMap()\n",
    "        self.data_dir = data_dir\n",
    "        self.index = []\n",
    "        doc_region_length = []\n",
    "        # self.output_dir = output_dir\n",
    "    \n",
    "    def parse_pairs(self):\n",
    "        '''\n",
    "        将对应的文本域中的单词文本转化为对应pair对\n",
    "        '''\n",
    "        td_pairs = []\n",
    "        for doc in os.listdir(self.data_dir):\n",
    "            new_path = os.path.join(self.data_dir,doc)\n",
    "            doc_id = self.doc_id_map[new_path]\n",
    "            for term in doc.split(' '):\n",
    "                td_pairs.append((self.term_id_map[term],str(doc_id)+\"_head\"))\n",
    "            first_flag = True\n",
    "            for line in open(new_path).readlines():\n",
    "                if first_flag:\n",
    "                    for term in line.strip().split(' '):\n",
    "                        td_pairs.append((self.term_id_map[term], str(doc_id)+\"_author\"))\n",
    "                    first_flag = False\n",
    "                else:\n",
    "                    for term in line.strip().split(' '):\n",
    "                        td_pairs.append((self.term_id_map[term], str(doc_id)+\"_body\"))\n",
    "        return td_pairs\n",
    "\n",
    "    def parse_index(self,td_pairs):\n",
    "        '''\n",
    "        将pair对中的一些单词和对应的文本提取出来变为倒排索引\n",
    "        对应的倒排索引格式:\n",
    "        [(0, ['0_head_1', '2_body_3', '3_body_2', '4_body_1', '6_body_1']),\n",
    "         (1, ['0_head_1']),\n",
    "         (2, ['0_head_1']),\n",
    "         (3, ['0_author_1', '1_author_1', '2_author_1', '7_author_1']),\n",
    "         ...\n",
    "        ]\n",
    "        '''\n",
    "        self.index = []\n",
    "        term_id = -1\n",
    "        doc = \"\"\n",
    "        num = 0\n",
    "        posting_list = []\n",
    "        for pair in sorted(td_pairs):\n",
    "            # 按对应的term进行建立索引\n",
    "            # print(pair)\n",
    "            if pair[0] != term_id:\n",
    "                if term_id != -1:\n",
    "                    posting_list.append(doc+\"_\"+str(num))\n",
    "                    self.index.append((term_id, posting_list))\n",
    "                    doc = \"\"\n",
    "                    num = 0\n",
    "                term_id = pair[0]\n",
    "                posting_list = []\n",
    "            if pair[1] != doc:\n",
    "                if doc != \"\":\n",
    "                    posting_list.append(doc+\"_\"+str(num))\n",
    "                    num = 0\n",
    "                doc = pair[1]\n",
    "            num += 1\n",
    "        posting_list.append(doc+\"_\"+str(num))\n",
    "        self.index.append((term_id, posting_list))\n",
    "        return self.index\n",
    "\n",
    "    def region_retrieve(self, query_list, region):\n",
    "        '''\n",
    "        支持域查询, 根据出现查询中单词的文本所在域上的三维空间模型的计算结果来进行返回结果排序\n",
    "        不同域的查询思路是: 在不同域中进行计算相似度, 然后将相关性按0.5 0.3 0.2进行累加最后排序\n",
    "        '''\n",
    "        region_list = []\n",
    "        query_doc_sim = [0]*len(os.listdir(data_path))\n",
    "        if region == 1:\n",
    "            region_list = [\"head\"]\n",
    "            query = query_list[0]\n",
    "            query_doc_sim = self.single_region_retrieve(query, region)\n",
    "        elif region == 2:\n",
    "            region_list = [\"author\"]\n",
    "            query = query_list[1]\n",
    "            query_doc_sim = self.single_region_retrieve(query, region)\n",
    "        elif region == 3:\n",
    "            region_list = [\"head\", \"author\"]\n",
    "            query1 = query_list[0]\n",
    "            query2 = query_list[1]\n",
    "            query_doc_sim1 = self.single_region_retrieve(query1, 1)\n",
    "            query_doc_sim2 = self.single_region_retrieve(query2, 2)\n",
    "            for i in range(len(query_doc_sim1)):\n",
    "                query_doc_sim[i] = query_doc_sim1[i]+query_doc_sim2[i]\n",
    "        elif region == 4:\n",
    "            region_list = [\"body\"]\n",
    "            query = query_list[2]\n",
    "            query_doc_sim = self.single_region_retrieve(query, region)\n",
    "        elif region == 5:\n",
    "            region_list = [\"head\", \"body\"]\n",
    "            query1 = query_list[0]\n",
    "            query2 = query_list[2]\n",
    "            query_doc_sim1 = self.single_region_retrieve(query1, 1)\n",
    "            query_doc_sim2 = self.single_region_retrieve(query2, 4)\n",
    "            for i in range(len(query_doc_sim1)):\n",
    "                query_doc_sim[i] = query_doc_sim1[i]+query_doc_sim2[i]\n",
    "        elif region == 6:\n",
    "            region_list = [\"author\", \"body\"]\n",
    "            query1 = query_list[1]\n",
    "            query2 = query_list[2]\n",
    "            query_doc_sim1 = self.single_region_retrieve(query1, 2)\n",
    "            query_doc_sim2 = self.single_region_retrieve(query2, 4)\n",
    "            for i in range(len(query_doc_sim1)):\n",
    "                query_doc_sim[i] = query_doc_sim1[i]+query_doc_sim2[i]\n",
    "        elif region == 7:\n",
    "            region_list = [\"head\",\"author\", \"body\"]\n",
    "            query0 = query_list[0]\n",
    "            query1 = query_list[1]\n",
    "            query2 = query_list[2]\n",
    "            query_doc_sim0 = self.single_region_retrieve(query0, 1)\n",
    "            query_doc_sim1 = self.single_region_retrieve(query1, 2)\n",
    "            query_doc_sim2 = self.single_region_retrieve(query2, 4)\n",
    "            for i in range(len(query_doc_sim1)):\n",
    "                query_doc_sim[i] = query_doc_sim1[i]+query_doc_sim2[i]+query_doc_sim0[i]\n",
    "        else:\n",
    "            # 默认为三维空间检索\n",
    "            # 这里给出一个接口\n",
    "            self.all_retrieve(query_list)\n",
    "            return\n",
    "        \n",
    "        sorted_id = sorted(range(len(query_doc_sim)), key=lambda k: query_doc_sim[k], reverse=True)\n",
    "        for i in range(len(sorted_id)):\n",
    "            print(\"排名第\"+str(i+1)+\"位的是文档D\"+str(sorted_id[i]+1)+\", 与Query余弦相似度SC(Q, D\"+str(sorted_id[i]+1)+\") :\",query_doc_sim[sorted_id[i]])\n",
    "\n",
    "    def single_region_retrieve(self, query, region):\n",
    "        ''' \n",
    "        单个域进行查询的接口\n",
    "        ''' \n",
    "        if region == 1:\n",
    "            region_list = \"head\"\n",
    "        elif region == 2:\n",
    "            region_list = \"author\"\n",
    "        elif region == 4:\n",
    "            region_list = \"body\"\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        # 先进行一个词干还原\n",
    "        \n",
    "        words = query.split(' ')\n",
    "        # print(words)\n",
    "        word_list = []\n",
    "        tagged_sent = pos_tag(words)     # 获取单词词性\n",
    "        wnl = WordNetLemmatizer()\n",
    "        for tag in tagged_sent:\n",
    "            wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "            word_list.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) # 词形还原\n",
    "        query = ' '.join(word_list)\n",
    "        query = query.lower().strip().translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "        # 对应的idf\n",
    "        term_doc_num = self.cal_idf()\n",
    "        doc_region_length = self.cal_doc_region_length(region)\n",
    "        # 进行计算获得query的向量\n",
    "        query_word = query.split(\" \")\n",
    "        query_vec = []\n",
    "        for i in range(len(set(query_word))):\n",
    "            query_vec.append(0)\n",
    "            for j in range(len(query_word)):\n",
    "                if query_word[j] == list(set(query_word))[i]:\n",
    "                    query_vec[i] += 1\n",
    "        for i in range(len(query_vec)):\n",
    "            query_vec[i] = round(math.log10(query_vec[i]+1),4)\n",
    "        # print(query_vec)\n",
    "        # 下面进行文档的向量构建\n",
    "        all_doc_vec = []\n",
    "        doc_num = len(os.listdir(self.data_dir))\n",
    "        for i in range(doc_num):\n",
    "            doc_vec = []\n",
    "            for j in range(len(set(query_word))):\n",
    "                # 找到对应的词项id\n",
    "                word_id = self.term_id_map[list(set(query_word))[j]] \n",
    "                # print(list(set(query_word))[j])\n",
    "                doc_vec.append(0)\n",
    "                if word_id < len(self.index):\n",
    "                    posting_list = self.index[word_id]\n",
    "                    # print(posting_list)\n",
    "                    for post in posting_list[1]:\n",
    "                        if post.split('_')[0] == str(i) and post.split('_')[1] == region_list:\n",
    "                            doc_vec[j] += int(post.split('_')[2])\n",
    "                    doc_vec[j] = round(math.log10(doc_vec[j]+1),4)*term_doc_num[word_id]\n",
    "            all_doc_vec.append(doc_vec)\n",
    "        # print(all_doc_vec)\n",
    "        all_doc_vec.append(query_vec)\n",
    "        ans = pd.DataFrame(all_doc_vec)\n",
    "        query_doc_sim = []\n",
    "        for i in range(len(all_doc_vec)-1):\n",
    "            query_doc_sim.append((ans.loc[i,:]*ans.loc[9,:]).sum()/doc_region_length[i])\n",
    "        return query_doc_sim\n",
    "            # print(\"Query和文档D\"+str(i+1)+\"的相似度SC(Q, D\"+str(i+1)+\") :\", (ans.loc[i,:]*ans.loc[9,:]).sum())\n",
    "        # sorted_id = sorted(range(len(query_doc_sim)), key=lambda k: query_doc_sim[k], reverse=True)\n",
    "        # for i in range(len(sorted_id)):\n",
    "        #     print(\"排名第\"+str(i+1)+\"位的是文档D\"+str(sorted_id[i]+1)+\", 与Query余弦相似度SC(Q, D\"+str(sorted_id[i]+1)+\") :\",query_doc_sim[sorted_id[i]])\n",
    "\n",
    "\n",
    "    def all_retrieve(self, query):\n",
    "        ''' \n",
    "        计算tf的方式: 词项在文档中出现的次数, tf在原始的值的基础上进行计算 减少文档长度的影响tf = log10(N+1)\n",
    "        计算余弦相似度, 将对应文本向量中的每一项用tf * idf来进行表示, \n",
    "        具体的计算方法: 用对应的在query中出现的term的query的tf*idf * 对应的doc的term的tf*idf 再除以对应doc的向量的长度\n",
    "        ''' \n",
    "        # 先进行一个词干还原\n",
    "        words = query.split(' ')\n",
    "        # print(words)\n",
    "        word_list = []\n",
    "        tagged_sent = pos_tag(words)     # 获取单词词性\n",
    "        wnl = WordNetLemmatizer()\n",
    "        for tag in tagged_sent:\n",
    "            wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "            word_list.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) # 词形还原\n",
    "        query = ' '.join(word_list)\n",
    "        query = query.lower().strip().translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "        # 对应的idf\n",
    "        term_doc_num = self.cal_idf()\n",
    "        self.cal_doc_length()\n",
    "        # 进行计算获得query的向量\n",
    "        query_word = query.split(\" \")\n",
    "        query_vec = []\n",
    "        for i in range(len(set(query_word))):\n",
    "            query_vec.append(0)\n",
    "            for j in range(len(query_word)):\n",
    "                if query_word[j] == list(set(query_word))[i]:\n",
    "                    query_vec[i] += 1\n",
    "        for i in range(len(query_vec)):\n",
    "            query_vec[i] = round(math.log10(query_vec[i]+1),4)\n",
    "        # print(query_vec)\n",
    "        # 下面进行文档的向量构建\n",
    "        all_doc_vec = []\n",
    "        doc_num = len(os.listdir(self.data_dir))\n",
    "        for i in range(doc_num):\n",
    "            doc_vec = []\n",
    "            for j in range(len(set(query_word))):\n",
    "                # 找到对应的词项id\n",
    "                word_id = self.term_id_map[list(set(query_word))[j]] \n",
    "                # print(list(set(query_word))[j])\n",
    "                doc_vec.append(0)\n",
    "                if word_id < len(self.index):\n",
    "                    posting_list = self.index[word_id]\n",
    "                    # print(posting_list)\n",
    "                    for post in posting_list[1]:\n",
    "                        if post.split('_')[0] == str(i):\n",
    "                            doc_vec[j] += int(post.split('_')[2])\n",
    "                    doc_vec[j] = round(math.log10(doc_vec[j]+1),4)*term_doc_num[word_id]\n",
    "            all_doc_vec.append(doc_vec)\n",
    "        # print(all_doc_vec)\n",
    "        all_doc_vec.append(query_vec)\n",
    "        ans = pd.DataFrame(all_doc_vec)\n",
    "        query_doc_sim = []\n",
    "        for i in range(len(all_doc_vec)-1):\n",
    "            query_doc_sim.append((ans.loc[i,:]*ans.loc[9,:]).sum()/self.doc_length[i])\n",
    "            # print(\"Query和文档D\"+str(i+1)+\"的相似度SC(Q, D\"+str(i+1)+\") :\", (ans.loc[i,:]*ans.loc[9,:]).sum())\n",
    "        sorted_id = sorted(range(len(query_doc_sim)), key=lambda k: query_doc_sim[k], reverse=True)\n",
    "        for i in range(len(sorted_id)):\n",
    "            print(\"排名第\"+str(i+1)+\"位的是文档D\"+str(sorted_id[i]+1)+\", 与Query余弦相似度SC(Q, D\"+str(sorted_id[i]+1)+\") :\",query_doc_sim[sorted_id[i]])\n",
    "\n",
    "    def cal_idf(self):\n",
    "        ''' \n",
    "        计算idf的接口 计算idf的方式: 出现该词项的所有文档的数目df idf = log(N/df)\n",
    "        ''' \n",
    "        doc_num = len(os.listdir(self.data_dir))\n",
    "        term_doc_num = []\n",
    "        for index in self.index:\n",
    "            doc_list = []\n",
    "            for doc in index[1]:\n",
    "                doc_list.append(doc.split(\"_\")[0])\n",
    "            term_doc_num.append(len(set(doc_list)))\n",
    "            # print(term_doc_num)\n",
    "        for i in range(len(term_doc_num)):\n",
    "            term_doc_num[i] = round(math.log10(doc_num/term_doc_num[i]),4)\n",
    "        return term_doc_num\n",
    "\n",
    "    def cal_doc_length(self):\n",
    "        ''' \n",
    "        计算向量的长度\n",
    "        ''' \n",
    "        doc_num = len(os.listdir(self.data_dir))\n",
    "        term_doc_num = self.cal_idf()\n",
    "        # 初始化列表\n",
    "        self.doc_length = [0]*doc_num\n",
    "        # !不能这样子初始化 浅拷贝\n",
    "        #all_doc_vec = [[0] * len(self.index)] * doc_num\n",
    "        all_doc_vec = [[0 for i in range(len(self.index))] for j in range(doc_num)]\n",
    "        # print(all_doc_vec)\n",
    "        for index in self.index:\n",
    "            term_id = index[0]\n",
    "            postings = index[1]\n",
    "            # print(postings)\n",
    "            for post in postings:\n",
    "                post_id = post.split('_')[0]\n",
    "                post_num = post.split('_')[2]\n",
    "                all_doc_vec[int(post_id)][int(term_id)] += int(post_num)\n",
    "        for doc_id in range(len(all_doc_vec)):\n",
    "            for term_id in range(len(doc)):\n",
    "                all_doc_vec[doc_id][term_id] *= term_doc_num[term_id]\n",
    "            for i in range(len(all_doc_vec[doc_id])):\n",
    "                self.doc_length[doc_id] += math.pow(all_doc_vec[doc_id][i], 2)\n",
    "            self.doc_length[doc_id] = round(math.sqrt(self.doc_length[doc_id]),4)\n",
    "        return self.doc_length\n",
    "\n",
    "    def cal_doc_region_length(self, region):\n",
    "        ''' \n",
    "        计算向量的长度\n",
    "        ''' \n",
    "        if region == 1:\n",
    "            region_list = \"head\"\n",
    "        elif region == 2:\n",
    "            region_list = \"author\"\n",
    "        elif region == 4:\n",
    "            region_list = \"body\"\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        doc_num = len(os.listdir(self.data_dir))\n",
    "        term_doc_num = self.cal_idf()\n",
    "        # 初始化列表\n",
    "        doc_region_length = [0]*doc_num\n",
    "        all_doc_vec = [[0 for i in range(len(self.index))] for j in range(doc_num)]\n",
    "        # print(all_doc_vec)\n",
    "        for index in self.index:\n",
    "            term_id = index[0]\n",
    "            postings = index[1]\n",
    "            # print(postings)\n",
    "            for post in postings:\n",
    "                post_id = post.split('_')[0]\n",
    "                post_num = post.split('_')[2]\n",
    "                if post.split('_')[1] == region_list:\n",
    "                    all_doc_vec[int(post_id)][int(term_id)] += int(post_num)\n",
    "        for doc_id in range(len(all_doc_vec)):\n",
    "            for term_id in range(len(doc)):\n",
    "                all_doc_vec[doc_id][term_id] *= term_doc_num[term_id]\n",
    "            for i in range(len(all_doc_vec[doc_id])):\n",
    "                doc_region_length[doc_id] += math.pow(all_doc_vec[doc_id][i], 2)\n",
    "            doc_region_length[doc_id] = round(math.sqrt(doc_region_length[doc_id]),4)\n",
    "        return doc_region_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "排名第1位的是文档D1, 与Query余弦相似度SC(Q, D1) : 0.06294704689092763\n",
      "排名第2位的是文档D2, 与Query余弦相似度SC(Q, D2) : 0.0\n",
      "排名第3位的是文档D3, 与Query余弦相似度SC(Q, D3) : 0.0\n",
      "排名第4位的是文档D4, 与Query余弦相似度SC(Q, D4) : 0.0\n",
      "排名第5位的是文档D5, 与Query余弦相似度SC(Q, D5) : 0.0\n",
      "排名第6位的是文档D6, 与Query余弦相似度SC(Q, D6) : 0.0\n",
      "排名第7位的是文档D7, 与Query余弦相似度SC(Q, D7) : 0.0\n",
      "排名第8位的是文档D8, 与Query余弦相似度SC(Q, D8) : 0.0\n",
      "排名第9位的是文档D9, 与Query余弦相似度SC(Q, D9) : 0.0\n"
     ]
    }
   ],
   "source": [
    "# 这是一个测试代码\n",
    "data_path = \".//newdata//\"\n",
    "regionIndex = RegionIndex(data_path)\n",
    "# regionIndex.parse_pairs()\n",
    "regionIndex.parse_index(regionIndex.parse_pairs())\n",
    "regionIndex.cal_idf()\n",
    "# regionIndex.cal_doc_length()\n",
    "query_list = [\"song\",\"dd\",\"\"]\n",
    "regionIndex.region_retrieve(query_list,3)\n",
    "# regionIndex.region_retrieve(\"Wine comes\", 7)\n",
    "# regionIndex.cal_doc_length()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____           _                _____      _        _                 _ \n",
      "|  __ \\         | |              |  __ \\    | |      (_)               | |\n",
      "| |__) |__   ___| |_ _ __ _   _  | |__) |___| |_ _ __ _  _____   ____ _| |\n",
      "|  ___/ _ \\ / _ \\ __| '__| | | | |  _  // _ \\ __| '__| |/ _ \\ \\ / / _` | |\n",
      "| |  | (_) |  __/ |_| |  | |_| | | | \\ \\  __/ |_| |  | |  __/\\ V / (_| | |\n",
      "|_|   \\___/ \\___|\\__|_|   \\__, | |_|  \\_\\___|\\__|_|  |_|\\___| \\_/ \\__,_|_|\n",
      "                           __/ |                                          \n",
      "                          |___/                                           \n",
      "    \n",
      "-----------------------------正在进行倒排索引构建-------------------------------\n",
      "-----------------------------正在构建倒排索引的IDF-------------------------------\n",
      "-----------------------------正在计算倒排索引文档向量长度-------------------------------\n",
      "-----------------------------构建完成, 下面进行按域检索-------------------------------\n",
      "请选择你要检索的域: \n",
      "1.诗歌名 2.诗人 3.诗歌内容 4.诗歌名和诗人 5.诗歌名和诗歌内容 6.诗人和诗歌内容 7.诗歌名和诗人和诗歌内容 8.全文本检索 9.退出\n",
      "诗歌名域查询语句:  freedom\n",
      "诗人域查询语句:  Rabindranath\n",
      "排名第1位的是文档D4, 与Query余弦相似度SC(Q, D4) : 0.11701697914979495\n",
      "排名第2位的是文档D5, 与Query余弦相似度SC(Q, D5) : 0.03056550494979494\n",
      "排名第3位的是文档D9, 与Query余弦相似度SC(Q, D9) : 0.03056550494979494\n",
      "排名第4位的是文档D1, 与Query余弦相似度SC(Q, D1) : 0.0\n",
      "排名第5位的是文档D2, 与Query余弦相似度SC(Q, D2) : 0.0\n",
      "排名第6位的是文档D3, 与Query余弦相似度SC(Q, D3) : 0.0\n",
      "排名第7位的是文档D6, 与Query余弦相似度SC(Q, D6) : 0.0\n",
      "排名第8位的是文档D7, 与Query余弦相似度SC(Q, D7) : 0.0\n",
      "排名第9位的是文档D8, 与Query余弦相似度SC(Q, D8) : 0.0\n",
      "-----------------------------当前查询完毕, 下面新一轮查询-------------------------------\n",
      "请选择你要检索的域: \n",
      "1.诗歌名 2.诗人 3.诗歌内容 4.诗歌名和诗人 5.诗歌名和诗歌内容 6.诗人和诗歌内容 7.诗歌名和诗人和诗歌内容 8.全文本检索 9.退出\n",
      "诗歌名域查询语句:  leave\n",
      "诗人域查询语句:  Yeats\n",
      "诗歌内容域查询语句:  loved the pilgrim\n",
      "排名第1位的是文档D8, 与Query余弦相似度SC(Q, D8) : 0.07147454233395972\n",
      "排名第2位的是文档D5, 与Query余弦相似度SC(Q, D5) : 0.06113100989958988\n",
      "排名第3位的是文档D3, 与Query余弦相似度SC(Q, D3) : 0.058670465936820994\n",
      "排名第4位的是文档D1, 与Query余弦相似度SC(Q, D1) : 0.05856393566167912\n",
      "排名第5位的是文档D2, 与Query余弦相似度SC(Q, D2) : 0.05231093803278688\n",
      "排名第6位的是文档D6, 与Query余弦相似度SC(Q, D6) : 0.008159903075550857\n",
      "排名第7位的是文档D4, 与Query余弦相似度SC(Q, D4) : 0.0\n",
      "排名第8位的是文档D7, 与Query余弦相似度SC(Q, D7) : 0.0\n",
      "排名第9位的是文档D9, 与Query余弦相似度SC(Q, D9) : 0.0\n",
      "-----------------------------当前查询完毕, 下面新一轮查询-------------------------------\n",
      "请选择你要检索的域: \n",
      "1.诗歌名 2.诗人 3.诗歌内容 4.诗歌名和诗人 5.诗歌名和诗歌内容 6.诗人和诗歌内容 7.诗歌名和诗人和诗歌内容 8.全文本检索 9.退出\n",
      "-----------------------------感谢您的使用,程序退出-------------------------------\n",
      "-----------------------------byebye q(≧▽≦q)-------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 这是主函数\n",
    "if __name__ == \"__main__\":\n",
    "    # 数据路径\n",
    "    data_path = \".//newdata//\"\n",
    "    # 初始化类\n",
    "    myIndex = RegionIndex(data_path)\n",
    "    print('''\n",
    " _____           _                _____      _        _                 _ \n",
    "|  __ \\         | |              |  __ \\    | |      (_)               | |\n",
    "| |__) |__   ___| |_ _ __ _   _  | |__) |___| |_ _ __ _  _____   ____ _| |\n",
    "|  ___/ _ \\ / _ \\ __| '__| | | | |  _  // _ \\ __| '__| |/ _ \\ \\ / / _` | |\n",
    "| |  | (_) |  __/ |_| |  | |_| | | | \\ \\  __/ |_| |  | |  __/\\ V / (_| | |\n",
    "|_|   \\___/ \\___|\\__|_|   \\__, | |_|  \\_\\___|\\__|_|  |_|\\___| \\_/ \\__,_|_|\n",
    "                           __/ |                                          \n",
    "                          |___/                                           \n",
    "    ''')\n",
    "    print(\"-----------------------------正在进行倒排索引构建-------------------------------\")\n",
    "    pairs = myIndex.parse_pairs()\n",
    "    myIndex.parse_index(pairs)\n",
    "    print(\"-----------------------------正在构建倒排索引的IDF-------------------------------\")\n",
    "    myIndex.cal_idf()\n",
    "    print(\"-----------------------------正在计算倒排索引文档向量长度-------------------------------\")\n",
    "    myIndex.cal_doc_length()\n",
    "    print(\"-----------------------------构建完成, 下面进行按域检索-------------------------------\")\n",
    "    while True:\n",
    "        print(\"请选择你要检索的域: \")\n",
    "        print(\"1.诗歌名 2.诗人 3.诗歌内容 4.诗歌名和诗人 5.诗歌名和诗歌内容 6.诗人和诗歌内容 7.诗歌名和诗人和诗歌内容 8.全文本检索 9.退出\")\n",
    "        a = input(\"请选择选项: \")\n",
    "        if a == \"1\":\n",
    "            query = input(\"请输入诗歌名域查询语句: \")\n",
    "            print(\"诗歌名域查询语句: \",query)\n",
    "            query_list = [query,\"\",\"\"]\n",
    "            myIndex.region_retrieve(query_list,1)\n",
    "        elif a == \"2\":\n",
    "            query = input(\"请输入诗人域查询语句: \")\n",
    "            print(\"诗人域查询语句: \",query)\n",
    "            query_list = [\"\",query,\"\"]\n",
    "            myIndex.region_retrieve(query_list,2)\n",
    "        elif a == \"3\":\n",
    "            query = input(\"请输入诗歌内容域查询语句: \")\n",
    "            print(\"诗歌内容域查询语句: \",query)\n",
    "            query_list = [\"\",\"\",query]\n",
    "            myIndex.region_retrieve(query_list,4)\n",
    "        elif a == \"4\":\n",
    "            query1 = input(\"请输入诗歌名域查询语句: \")\n",
    "            print(\"诗歌名域查询语句: \",query1)\n",
    "            query2 = input(\"请输入诗人域查询语句: \")\n",
    "            print(\"诗人域查询语句: \",query2)\n",
    "            query_list = [query1,query2,\"\"]\n",
    "            myIndex.region_retrieve(query_list,3)\n",
    "        elif a == \"5\":\n",
    "            query1 = input(\"请输入诗歌名域查询语句: \")\n",
    "            print(\"诗歌名域查询语句: \",query1)\n",
    "            query2 = input(\"请输入诗歌内容域查询语句: \")\n",
    "            print(\"诗歌内容域查询语句: \",query2)\n",
    "            query_list = [query1,\"\",query2]\n",
    "            myIndex.region_retrieve(query_list,5)\n",
    "        elif a == \"6\":\n",
    "            query1 = input(\"请输入诗人域查询语句: \")\n",
    "            print(\"诗人域查询语句: \",query1)\n",
    "            query2 = input(\"请输入诗歌内容域查询语句: \")\n",
    "            print(\"诗歌内容域查询语句: \",query2)\n",
    "            query_list = [\"\",query1,query2]\n",
    "            myIndex.region_retrieve(query_list,6)\n",
    "        elif a == \"7\":\n",
    "            query0 = input(\"请输入诗歌名域查询语句: \")\n",
    "            print(\"诗歌名域查询语句: \",query0)\n",
    "            query1 = input(\"请输入诗人域查询语句: \")\n",
    "            print(\"诗人域查询语句: \",query1)\n",
    "            query2 = input(\"请输入诗歌内容域查询语句: \")\n",
    "            print(\"诗歌内容域查询语句: \",query2)\n",
    "            query_list = [query0,query1,query2]\n",
    "            myIndex.region_retrieve(query_list,7)\n",
    "        elif a == \"8\":\n",
    "            query = input(\"请输入查询语句: \")\n",
    "            myIndex.all_retrieve(query)\n",
    "        elif a == \"9\":\n",
    "            print(\"-----------------------------感谢您的使用,程序退出-------------------------------\")\n",
    "            print(\"-----------------------------byebye q(≧▽≦q)-------------------------------\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"-----------------------------您输入错误啦~~~~ ::>_<::-------------------------------\")\n",
    "            print(\"-----------------------------程序退出-------------------------------\")\n",
    "            break\n",
    "        print(\"-----------------------------当前查询完毕, 下面新一轮查询-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92c5ccf98f3573c0fc075231a5a38cb399f6493ca6d9132f35f1e99ee9a222a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
